# üîó 4 Different Unsupervised Algorithms for our Data and their Analysis:

### 1. K-Means:
K-Means Clustering is a form of unsupervised machine learning in the form of vector quantization that works by partitioning an amount of observations into clusters, in which each observation belongs to a cluster with the closest mean to the cluster‚Äôs center/centroid. 
For our evaluation, we will be having a look at the Inertia of the K-Means result. For K-means, a good model has a low value of inertia and a low number of clusters. In our test, the amount of cluster (K-value) that we chose to use in the end based on the elbow test is 2 and after implementing said K-value to our data into, we get the results and the inertia value of 2.059497010801236e+39. This shows that the algorithm could be improved, and this is due to the fact all the features used are all numerical and that could lead to low accuracy of the algorithm.

### 2. K-Modes:
K-Modes is a variation of the K-Means algorithm designed for categorical data, making it a suitable choice for our data with all categorical columns. K-Modes assigns data points to clusters based on the mode (most frequent category) within each cluster. And  Since our dataset consists entirely of categorical variables (such as ‚Äògeo_op_status‚Äô, ‚Äòservice_source_sgmnt‚Äô, ‚Äòbrowser_segment‚Äô, ‚Äòpublisher_segment‚Äô, ‚Äòadnet_segment‚Äô), K-Modes was well-suited for this data type.
For the evaluation of this algorithm, we‚Äôll be looking at the Silhouette_score. A ‚ÄúSilhouette Coefficient‚Äù measures the cohesion among data points in a cluster; the smaller it is the better. The average silhouette score of a dataset is between -1 and 1. From our test, we received a Silhouette score of 0.6365263130069132. A silhouette score of 0.6365 is relatively high, suggesting that the clusters are well-separated. Higher silhouette scores are desirable as they indicate better-defined clusters

### 3. PCA (Principal Component Analysis):
Principal Component Analysis (PCA) is a technique for lowering the number of independent variables in a dataset, which is especially useful when the ratio of data points to independent variables is low. When it comes to maximizing our targeted tactics, PCA is essential. In order to get a variable that expresses the highest variance within a linear combination of factors, PCA transforms the combination of variables. Our objective to simplify the complexity involved in user behavior analysis is perfectly aligned with PCA's ability to reduce the number of independent variables in our dataset. 
Seeing as how PCA works involves reducing the amount of variables from the input, all the while retaining the essential information of the dataset, one key thing to evaluate the model‚Äôs effectiveness is by its reconstruction error, to see if the dimensionality reduction affects the overall meaning of the information after reconstructing the data from the reduced dataset. A smaller value of reconstruction error means that the reduced data still captures the essential information well. As per our test, the reconstruction error that we got is 6.967170966695865e-24, which is a fairly low amount of error. 

### 4. SVD (Singular value decomposition):
SVD (Singular value decomposition) is  a method and a fundamental concept in linear algebra and numerical analytics that essentially works by decomposing a matrix into 3 matrices. For customer segmentation cases, SVD works by grouping customers with similar characteristics/behavior together. Similar goes to ads targeting. 
For SVD, we evaluate this method by calculating its variance ratio. This evaluation involves examining the proportion of the total variance explained by a subset of singular values. From our test, we received a variance ratio of [0.51389945, 0.24987777, 0.16963612] and the cumulative explained variance ratio [0.58114889 0.78156426 0.93258162 0.98799175]. 
